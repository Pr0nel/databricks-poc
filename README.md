# Lakehouse POC: Hybrid Data Architecture

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Apache Spark 3.5.0](https://img.shields.io/badge/apache%20spark-3.5.0-orange.svg)](https://spark.apache.org/)
[![Delta Lake 3.1.0](https://img.shields.io/badge/delta%20lake-3.1.0-red.svg)](https://delta.io/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ Tabla de Contenidos

- [DescripciÃ³n](#descripciÃ³n)
- [Arquitectura](#arquitectura)
- [Stack TecnolÃ³gico](#stack-tecnolÃ³gico)
- [Requisitos Previos](#requisitos-previos)
- [InstalaciÃ³n](#instalaciÃ³n)
- [ConfiguraciÃ³n](#configuraciÃ³n)
- [Uso](#uso)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Etapas de ImplementaciÃ³n](#etapas-de-implementaciÃ³n)
- [Troubleshooting](#troubleshooting)
- [Contribuciones](#contribuciones)
- [Licencia](#licencia)

---

## ğŸ“– DescripciÃ³n

**Lakehouse POC** es una prueba de concepto que demuestra una arquitectura **data lakehouse hÃ­brida** local + cloud, combinando:

- âœ… **Streaming en tiempo real** con Apache Kafka
- âœ… **Delta Lake** para ACID transactions y time travel
- âœ… **Arquitectura Medallion** (Bronze â†’ Silver â†’ Gold)
- âœ… **Cloud Storage** (AWS S3)
- âœ… **Databricks Integration** con Unity Catalog y Auto Loader
- âœ… **Gobernanza de datos** y esquemas evolucionables

**PropÃ³sito:** Demostrar patrones enterprise reales de ingesta, transformaciÃ³n y gobernanza de datos.

---

## ğŸ—ï¸ Arquitectura

### Diagrama General

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LAKEHOUSE HÃBRIDO                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  CAPA 1: INGESTA (Docker Local - Streaming)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Kafka (localhost:9092) â†’ Spark Consumer â†’ Delta LOCAL  â”‚  â”‚
â”‚  â”‚ â””â”€ Formato: JSON                                       â”‚  â”‚
â”‚  â”‚ â””â”€ Frecuencia: Micro-batches (5s)                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â†“                                  â”‚
â”‚  CAPA 2: ALMACENAMIENTO (Batch - Persistencia)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Delta LOCAL â†’ S3 Parquet (particionado por fecha)      â”‚  â”‚
â”‚  â”‚ â””â”€ Formato: Parquet                                    â”‚  â”‚
â”‚  â”‚ â””â”€ Particionado: ingestion_date                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â†“                                  â”‚
â”‚  CAPA 3: TRANSFORMACIÃ“N (Databricks Cloud)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Auto Loader S3 â†’ Bronze â†’ Silver â†’ Gold                â”‚  â”‚
â”‚  â”‚ â””â”€ Schema Evolution automÃ¡tica                         â”‚  â”‚
â”‚  â”‚ â””â”€ Unity Catalog (gobernanza)                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Stack TecnolÃ³gico

| Componente | VersiÃ³n | PropÃ³sito |
|-----------|---------|----------|
| **Kafka** | 7.4.0 | Streaming de eventos |
| **Spark** | 3.5.0 | Procesamiento distribuido |
| **Delta Lake** | 3.1.0 | Data lakehouse (ACID, time travel) |
| **Python** | 3.9+ | OrquestaciÃ³n y lÃ³gica |
| **Docker** | 24.0+ | ContenedorizaciÃ³n |
| **AWS S3** | - | Cloud storage |
| **Databricks** | Serverless | Data platform cloud |

---

## ğŸ“¦ Requisitos Previos

### Local

- Python 3.9+
- Docker y Docker Compose 24.0+
- Java 11+ (para Spark)
- pip (gestor de paquetes Python)

### AWS

- Cuenta AWS con permisos S3
- IAM user con credenciales (Access Key + Secret Key)

### Databricks

- Workspace Databricks (trial gratuito disponible)
- PAT token (Personal Access Token)

---

## ğŸš€ InstalaciÃ³n

### 1. Clonar Repositorio

```bash
git clone https://github.com/tu-usuario/databricks-poc.git
cd databricks-poc
```

### 2. Crear Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

### 3. Instalar Dependencias

```bash
pip install -r requirements.txt
```

### 4. Configurar Variables de Entorno

```bash
cp .env.example .env
# Editar .env con tus credenciales
```

```bash
# .env
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_S3_BUCKET=your-lakehouse-poc-bucket
DATABRICKS_HOST=https://your-workspace.databricks.com
DATABRICKS_TOKEN=your_pat_token
```

---

## âš™ï¸ ConfiguraciÃ³n

### Validar Infraestructura (ETAPA 0)

```bash
# Verificar Python y dependencias
python3 config/settings.py

# Verificar logging
python3 config/logging_config.py

# Verificar Docker
scripts/docker-helpers.sh status
```

### Levantar Docker (Kafka + Zookeeper)

```bash
# Desarrollo (sin persistencia)
scripts/docker-helpers.sh dev-up

# Verificar que estÃ¡ listo
scripts/docker-helpers.sh status

# Testear conexiÃ³n
scripts/docker-helpers.sh test-kafka
```

---

## ğŸ’» Uso

### EjecuciÃ³n Completa del Pipeline

```bash
# AsegÃºrate de que Docker estÃ¡ activo
scripts/docker-helpers.sh dev-up

# Ejecutar todo (5 etapas)
python3 run_pipeline.py
```

**Resultado esperado:**
```
===================================================================
    LAKEHOUSE POC - PIPELINE COMPLETO
===================================================================

[PASO 1/5] Setup S3 Structure
    Setup S3 structure (crear carpetas bronze/silver/gold) - EXITOSO

[PASO 2/5] Kafka Producer
    Kafka Producer (generar 50 eventos) - EXITOSO
    ProducciÃ³n completada
    Exitosos: 50/50
    Fallidos: 0/50

[PASO 3/5] Spark Streaming Consumer
    Streaming: Kafka â†’ Delta LOCAL (120 segundos) - EXITOSO
    Filas en Delta: 50

[PASO 4/5] Spark Batch Writer
    Batch: Delta LOCAL â†’ S3 Parquet - EXITOSO
    Datos escritos a S3 exitosamente

[PASO 5/5] Spark S3 Validator
    Validation: S3 Quality Checks - EXITOSO
    Total de filas: 50
    Data Quality Checks completados

===================================================================
    PIPELINE COMPLETADO EXITOSAMENTE
===================================================================
```

### EjecuciÃ³n de Componentes Individuales

```bash
# Solo Producer (generar eventos)
python3 scripts/kafka_producer.py

# Solo Consumer (ingestar a Delta)
python3 pyspark-jobs/01_spark_kafka_consumer.py

# Solo Batch (persistir a S3)
python3 pyspark-jobs/02_spark_delta_to_s3.py

# Solo ValidaciÃ³n
python3 pyspark-jobs/03_spark_s3_validator.py
```

### Docker Commands

```bash
# Levantar
scripts/docker-helpers.sh dev-up

# Ver estado
scripts/docker-helpers.sh status

# Ver logs de Kafka
scripts/docker-helpers.sh logs-kafka

# Acceder a shell de Kafka
scripts/docker-helpers.sh shell-kafka

# Reset completo (limpia todo)
scripts/docker-helpers.sh reset-dev

# Detener
scripts/docker-helpers.sh stop

# Ver todos los comandos
scripts/docker-helpers.sh help
```

---

## ğŸ“ Estructura del Proyecto

```
databricks-poc/
â”‚
â”œâ”€ config/                          â† ConfiguraciÃ³n centralizada
â”‚  â”œâ”€ config.yaml                   (vars de entorno)
â”‚  â”œâ”€ settings.py                   (parser de config)
â”‚  â””â”€ logging_config.py             (setup de logging)
â”‚
â”œâ”€ utils/                           â† MÃ³dulos reutilizables
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ health_check.py               (health checks)
â”‚  â”œâ”€ spark_utils.py                (factory SparkSession)
â”‚  â”œâ”€ retry_logic.py                (polÃ­ticas de retry)
â”‚  â”œâ”€ schema_definitions.py          (schemas centralizados)
â”‚  â””â”€ data_validators.py            (validaciones comunes)
â”‚
â”œâ”€ scripts/                         â† Herramientas de desarrollo
â”‚  â”œâ”€ kafka_producer.py             (genera eventos a Kafka)
â”‚  â”œâ”€ setup_s3.py                   (estructura S3)
â”‚  â””â”€ docker-helpers.sh             (CLI Docker)
â”‚
â”œâ”€ pyspark-jobs/                    â† Jobs de Spark
â”‚  â”œâ”€ 01_spark_kafka_consumer.py    (Kafka â†’ Delta)
â”‚  â”œâ”€ 02_spark_delta_to_s3.py       (Delta â†’ S3)
â”‚  â””â”€ 03_spark_s3_validator.py      (Validar S3)
â”‚
â”œâ”€ notebooks/                       â† Notebooks Databricks (ETAPA 4)
â”‚  â”œâ”€ 01_auto_loader_setup.py
â”‚  â”œâ”€ 02_auto_loader_bronze.py
â”‚  â””â”€ 03_schema_evolution_test.py
â”‚
â”œâ”€ docker/
â”‚  â”œâ”€ docker-compose.yml            (config principal)
â”‚  â””â”€ docker-compose.dev.yml        (override desarrollo)
â”‚
â”œâ”€ logs/                            â† Logs rotados
â”‚  â”œâ”€ orchestrator.log
â”‚  â”œâ”€ kafka_producer.log
â”‚  â”œâ”€ spark_kafka_consumer.log
â”‚  â””â”€ ...
â”‚
â”œâ”€ spark_checkpoints/               â† Checkpoints de Spark
â”‚  â”œâ”€ delta_consumer/
â”‚  â””â”€ s3_consumer/
â”‚
â”œâ”€ delta_tables/                    â† Delta local (temporal)
â”‚  â””â”€ events_raw/
â”‚
â”œâ”€ requirements.txt                 â† Dependencias Python
â”œâ”€ .env                             â† Template de variables
â”œâ”€ .gitignore
â”œâ”€ LICENSE
â”œâ”€ README.md                        â† Este archivo
â””â”€ run_pipeline.py                  â† Orquestador principal
```

---

## ğŸ“Š Etapas de ImplementaciÃ³n

### âœ… ETAPA 0: ValidaciÃ³n de Infraestructura
Verificar que todos los servicios estÃ¡n disponibles:
- Databricks Serverless
- AWS S3 + IAM
- Docker + Kafka
- Spark local + Delta

### âœ… ETAPA 1: Setup Inicial
Preparar cÃ³digo base sin ejecutar:
- Docker setup
- AWS S3 config
- Databricks setup

### âœ… ETAPA 2: Docker Kafka
Levantar Kafka y validar:
- Kafka cluster
- Producer de eventos
- Kafka topics

### âœ… ETAPA 3: Spark Streaming (ACTUAL)
Implementar pipeline local:
- `01_spark_kafka_consumer.py`: Kafka â†’ Delta
- `02_spark_delta_to_s3.py`: Delta â†’ S3
- `03_spark_s3_validator.py`: ValidaciÃ³n

### â³ ETAPA 4: Databricks Auto Loader
Implementar transformaciones cloud:
- Auto Loader setup
- Schema evolution
- Unity Catalog

### â³ ETAPA 5: Transformaciones Medallion
Implementar capas Silver + Gold:
- Bronze â†’ Silver (limpieza)
- Silver â†’ Gold (agregaciones)
- Lineage y gobernanza

### â³ ETAPA 6: DocumentaciÃ³n
Notebooks y docs finales

---

## ğŸ› Troubleshooting

### Kafka no responde

```bash
# Reset completo de Docker
scripts/docker-helpers.sh reset-dev

# Esperar 20 segundos

# Verificar
scripts/docker-helpers.sh test-kafka
```

### Error de credenciales AWS

```bash
# Verificar que .env estÃ¡ configurado
cat .env

# Validar acceso S3
python3 scripts/setup_s3.py
```

### Spark falla con timeout

Aumentar timeouts en config.yaml
    - spark_consumer: 180 â†’ 300 (segundos)

---

## ğŸ“ˆ MÃ©tricas y Monitoreo

Todos los jobs generan logs detallados en `logs/`:

```bash
# Ver logs en tiempo real
tail -f logs/spark_kafka_consumer.log

# Ver Ãºltimo 50 eventos
tail -50 logs/orchestrator.log

# Buscar errores
grep ERROR logs/*.log
```

---

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Para cambios principales:

1. Fork el proyecto
2. Crea una rama (`git checkout -b feature/mejora`)
3. Commit cambios (`git commit -m 'Agregar mejora'`)
4. Push a la rama (`git push origin feature/mejora`)
5. Open Pull Request

---

## ğŸ“š Recursos Adicionales

- [Delta Lake Documentation](https://docs.delta.io/)
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Databricks Unity Catalog](https://docs.databricks.com/en/data-governance/unity-catalog/index.html)
- [AWS S3 Guide](https://docs.aws.amazon.com/s3/)
- [Kafka Documentation](https://kafka.apache.org/documentation/)

---

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver archivo [LICENSE](LICENSE) para detalles.

---

## âœï¸ Autor

**[Pablo Ratache Rojas]** - Data Engineer

- GitHub: [@Pr0nel](https://github.com/Pr0nel)
- LinkedIn: [Pablo Ratache](www.linkedin.com/in/pablo-ratache-rojas-9a9602140)
- Portfolio: [Pablo's Portfolio](https://pr0nel.github.io/cv_pablo_ratache/)

---

## ğŸ¯ PrÃ³ximos Pasos

- [ ] Agregar tests unitarios
- [ ] Documentar transformaciones de negocios
- [ ] Configurar CI/CD con GitHub Actions

---

**Ãšltima actualizaciÃ³n:** Noviembre 2025

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver el archivo LICENSE para mÃ¡s detalles. Sino, en <https://opensource.org/license/mit>.